{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìö Workshop: RAG + LangChain + Streamlit\n",
        "\n",
        "## ‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà 3: Build RAG Pipeline with LangChain (1:30 ‚Äì 2:30)\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ\n",
        "- ‡πÉ‡∏ä‡πâ RetrievalQA ‡∏´‡∏£‡∏∑‡∏≠ ConversationalRetrievalChain\n",
        "- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ retriever (search_type=\"similarity\", k=3)\n",
        "- ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö LLM (Groq Llama)\n",
        "- ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°-‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
        "- ‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Prompt Template\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîó RAG Pipeline Overview\n",
        "\n",
        "### ‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏° RAG Pipeline:\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ] --> B[Embeddings]\n",
        "    B --> C[Vector Search]\n",
        "    D[Vector Store] --> C\n",
        "    C --> E[Retriever]\n",
        "    E --> F[Context Documents]\n",
        "    F --> G[Prompt Template]\n",
        "    G --> H[LLM]\n",
        "    H --> I[‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö]\n",
        "```\n",
        "\n",
        "### LangChain Chains:\n",
        "\n",
        "1. **RetrievalQA** - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°-‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢\n",
        "2. **ConversationalRetrievalChain** - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ memory\n",
        "3. **RetrievalQAWithSourcesChain** - ‡∏£‡∏ß‡∏°‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/game/01_Projects/workshop_rag_rmutl/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Libraries imported successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_40136/1761979277.py:36: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(\n",
            "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda:0\n",
            "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
            "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
            "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ‡πÇ‡∏´‡∏•‡∏î Vector Store ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: 13 documents\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries ‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î Vector Store\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# ‡πÄ‡∏û‡∏¥‡πà‡∏° path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö import modules\n",
        "sys.path.append('..')\n",
        "\n",
        "# Import LangChain components\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Import utilities\n",
        "from dotenv import load_dotenv\n",
        "import logging\n",
        "\n",
        "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ‡πÇ‡∏´‡∏•‡∏î environment variables\n",
        "load_dotenv()\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "\n",
        "# ‡πÇ‡∏´‡∏•‡∏î Vector Store (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)\n",
        "vectorstore_path = \"../vectorstore\"\n",
        "if os.path.exists(vectorstore_path):\n",
        "    try:\n",
        "        # ‡πÇ‡∏´‡∏•‡∏î embeddings\n",
        "        embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"all-MiniLM-L6-v2\",\n",
        "            cache_folder=\"../model_cache\"\n",
        "        )\n",
        "        \n",
        "        # ‡πÇ‡∏´‡∏•‡∏î FAISS vector store\n",
        "        vectorstore = FAISS.load_local(vectorstore_path, embeddings, allow_dangerous_deserialization=True)\n",
        "        print(f\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î Vector Store ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {vectorstore.index.ntotal} documents\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î Vector Store: {e}\")\n",
        "        vectorstore = None\n",
        "else:\n",
        "    print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö Vector Store - ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏±‡∏ô notebook ‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà 2 ‡∏Å‡πà‡∏≠‡∏ô\")\n",
        "    vectorstore = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LLM (Groq)\n",
        "\n",
        "### Groq ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n",
        "\n",
        "**Groq** ‡πÄ‡∏õ‡πá‡∏ô AI inference company ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£ LLM inference ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å\n",
        "\n",
        "### ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á Groq:\n",
        "- ‚ö° **‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å** - inference ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ OpenAI\n",
        "- üí∞ **‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ñ‡∏π‡∏Å** - ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ï‡πà‡∏≠ token ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤\n",
        "- üîì **Open Source** - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• open source\n",
        "- üåê **API** - ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡∏ú‡πà‡∏≤‡∏ô API\n",
        "\n",
        "### ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:\n",
        "- `llama-3.3-70b-versatile` - ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏ç‡πà, ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ\n",
        "- `llama-3.1-8b-instant` - ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏•‡πá‡∏Å, ‡πÄ‡∏£‡πá‡∏ß\n",
        "- `mixtral-8x7b-32768` - Mixtral model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Groq LLM ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n",
            "ü§ñ Model: llama-3.3-70b-versatile\n",
            "üå°Ô∏è Temperature: 0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö LLM: ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö/‡∏Ñ‡πà‡∏∞ ‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏Ñ‡∏£‡∏±‡∏ö/‡∏Ñ‡πà‡∏∞ ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ó‡∏µ‡πà...\n"
          ]
        }
      ],
      "source": [
        "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LLM (Groq)\n",
        "def setup_llm(model_name=\"llama-3.3-70b-versatile\", temperature=0.1):\n",
        "    \"\"\"‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Groq LLM\"\"\"\n",
        "    try:\n",
        "        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö API Key\n",
        "        api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise ValueError(\"GROQ_API_KEY ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ô environment variables\")\n",
        "        \n",
        "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Groq LLM\n",
        "        llm = ChatGroq(\n",
        "            groq_api_key=api_key,\n",
        "            model_name=model_name,\n",
        "            temperature=temperature\n",
        "        )\n",
        "        \n",
        "        print(f\"‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Groq LLM ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
        "        print(f\"ü§ñ Model: {model_name}\")\n",
        "        print(f\"üå°Ô∏è Temperature: {temperature}\")\n",
        "        \n",
        "        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö LLM\n",
        "        test_response = llm.invoke(\"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ\")\n",
        "        print(f\"üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö LLM: {test_response.content[:50]}...\")\n",
        "        \n",
        "        return llm\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n",
        "        return None\n",
        "\n",
        "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LLM\n",
        "llm = setup_llm(\"llama-3.3-70b-versatile\", temperature=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ ‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Prompt Template\n",
        "\n",
        "### ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Custom Prompt?\n",
        "\n",
        "1. **‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö** - ‡πÉ‡∏´‡πâ LLM ‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
        "2. **‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó** - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏Ç‡∏≠‡∏á LLM\n",
        "3. **‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û** - ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô\n",
        "4. **‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢** - ‡∏õ‡∏£‡∏±‡∏ö prompt ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n",
        "\n",
        "### ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt Template:\n",
        "\n",
        "```\n",
        "System Message: ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏Ç‡∏≠‡∏á LLM\n",
        "Context: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£\n",
        "Question: ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ\n",
        "Answer: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Prompt Template ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n",
            "üìù Prompt Template:\n",
            "--------------------------------------------------\n",
            "\n",
            "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡∏à‡∏á‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\n",
            "\n",
            "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:\n",
            "{context}\n",
            "\n",
            "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}\n",
            "\n",
            "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Prompt Template\n",
        "def create_custom_prompt():\n",
        "    \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Prompt Template ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\"\"\"\n",
        "    \n",
        "    prompt_template = \"\"\"\n",
        "        ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡∏à‡∏á‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\n",
        "\n",
        "        ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:\n",
        "        {context}\n",
        "\n",
        "        ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}\n",
        "\n",
        "        ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\"\"\"\n",
        "    \n",
        "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á PromptTemplate\n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Prompt Template ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
        "    print(\"üìù Prompt Template:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(prompt_template)\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    return PROMPT\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt Template\n",
        "custom_prompt = create_custom_prompt()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîó ‡∏™‡∏£‡πâ‡∏≤‡∏á RetrievalQA Chain\n",
        "\n",
        "### RetrievalQA Chain ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n",
        "\n",
        "**RetrievalQA** ‡πÄ‡∏õ‡πá‡∏ô chain ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°:\n",
        "1. **Retriever** - ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á\n",
        "2. **QA Chain** - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏î‡πâ\n",
        "\n",
        "### ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:\n",
        "- `llm`: Language Model\n",
        "- `retriever`: Vector Store Retriever\n",
        "- `chain_type`: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á chain (\"stuff\", \"map_reduce\", \"refine\")\n",
        "- `prompt`: Custom Prompt Template\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç ‡∏™‡∏£‡πâ‡∏≤‡∏á Retriever ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (k=3)\n",
            "‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á RetrievalQA Chain ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n",
            "üîó Chain Type: stuff\n",
            "üìä Return Source Documents: True\n"
          ]
        }
      ],
      "source": [
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á RetrievalQA Chain\n",
        "def create_retrieval_qa_chain(vectorstore, llm, custom_prompt, k=3):\n",
        "    \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á RetrievalQA Chain\"\"\"\n",
        "    try:\n",
        "        if not vectorstore or not llm:\n",
        "            raise ValueError(\"‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ vectorstore ‡πÅ‡∏•‡∏∞ llm\")\n",
        "        \n",
        "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á retriever\n",
        "        retriever = vectorstore.as_retriever(\n",
        "            search_type=\"similarity\",\n",
        "            search_kwargs={\"k\": k}\n",
        "        )\n",
        "        \n",
        "        print(f\"üîç ‡∏™‡∏£‡πâ‡∏≤‡∏á Retriever ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (k={k})\")\n",
        "        \n",
        "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á RetrievalQA chain\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=retriever,\n",
        "            chain_type_kwargs={\"prompt\": custom_prompt},\n",
        "            return_source_documents=True\n",
        "        )\n",
        "        \n",
        "        print(\"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á RetrievalQA Chain ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
        "        print(f\"üîó Chain Type: stuff\")\n",
        "        print(f\"üìä Return Source Documents: True\")\n",
        "        \n",
        "        return qa_chain\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n",
        "        return None\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á RetrievalQA Chain\n",
        "if vectorstore and llm and custom_prompt:\n",
        "    qa_chain = create_retrieval_qa_chain(vectorstore, llm, custom_prompt, k=3)\n",
        "else:\n",
        "    print(\"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á RetrievalQA Chain - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö vectorstore, llm, ‡πÅ‡∏•‡∏∞ prompt\")\n",
        "    qa_chain = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö RAG Pipeline\n",
        "\n",
        "### ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏î‡∏™‡∏≠‡∏ö:\n",
        "1. \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?\"\n",
        "2. \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?\"\n",
        "3. \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?\"\n",
        "4. \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏û‡∏∑‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_40136/4269863517.py:17: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = qa_chain({\"query\": question})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö RAG Pipeline\n",
            "============================================================\n",
            "\n",
            "üìù ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà 1: ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡∏Ç‡∏≠‡∏á‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô\n",
            "\n",
            "üìö ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á (3 ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£):\n",
            "  1. ‡∏´‡∏ô‡πâ‡∏≤ 2\n",
            "     ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: ‡∏™‡∏°‡∏±‡∏¢‡∏Å‡∏£‡∏∏‡∏á‡∏£‡∏±‡∏ï‡∏ô‡πÇ‡∏Å‡∏™‡∏¥‡∏ô‡∏ó‡∏£‡πå ‡∏ô‡∏Ñ‡∏£‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏ê‡∏≤‡∏ô‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏±‡∏ß‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏£‡∏≤‡∏ä ‡πÄ‡∏à‡πâ‡∏≤‡∏ú‡∏π‡πâ‡∏Ñ‡∏£‡∏≠‡∏á‡∏ô‡∏Ñ‡∏£‡∏ô‡πà‡∏≤‡∏ô‡πÉ‡∏ô‡∏ä‡∏±‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á\n",
            "‡∏ó‡∏∏‡∏Å‡∏≠‡∏á‡∏Ñ‡πå‡∏ï‡πà‡∏≤‡∏á‡∏õ‡∏è‡∏¥‡∏ö...\n",
            "  2. ‡∏´‡∏ô‡πâ‡∏≤ 0\n",
            "     ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: ‡∏Ñ‡∏£‡∏≠‡∏á‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏ó‡∏ô ‡∏à‡∏∂‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏™‡∏ô‡∏≤‡∏≠‡∏≥‡∏°‡∏≤‡∏ï‡∏¢‡πå‡πÑ‡∏õ‡πÄ‡∏ä‡∏¥‡∏ç ‡πÄ‡∏à‡πâ‡∏≤‡πÄ‡∏Å‡πâ‡∏≤‡πÄ‡∏ñ‡∏∑‡πà‡∏≠‡∏ô‡πÄ‡∏Å‡∏£‡∏á‡πÉ‡∏à‡∏õ‡∏π‡πà‡∏à‡∏∂‡∏á‡∏¢‡∏≠‡∏°‡πÑ‡∏õ‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞\n",
            "‡∏°‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏ä‡∏≤‡∏¢‡∏≤‡∏Ñ...\n",
            "  3. ‡∏´‡∏ô‡πâ‡∏≤ 2\n",
            "     ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: ‡∏™‡∏°‡∏±‡∏¢‡∏•‡πâ‡∏≤‡∏ô‡∏ô‡∏≤ \n",
            "‡πÉ‡∏ô‡∏õ‡∏µ ‡∏û.‡∏®. 1993 ‡∏û‡∏£‡∏∞‡πÄ‡∏à‡πâ‡∏≤‡∏ï‡∏¥‡πÇ‡∏•‡∏Å‡∏£‡∏≤‡∏ä‡∏Å‡∏©‡∏±‡∏ï‡∏£‡∏¥‡∏¢‡πå‡∏ô‡∏Ñ‡∏£‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏à‡∏∞‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏≠‡∏á\n",
            "‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÅ...\n",
            "\n",
            "============================================================\n",
            "\n",
            "üìù ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà 2: ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏´‡πà‡∏á ‡πÄ‡∏ä‡πà‡∏ô ‡∏ß‡∏±‡∏î‡∏û‡∏£‡∏∞‡∏ò‡∏≤‡∏ï‡∏∏‡πÅ‡∏ä‡πà‡πÅ‡∏´‡πâ‡∏á, ‡∏ß‡∏±‡∏î‡∏™‡∏ß‡∏ô‡∏ï‡∏≤‡∏•, ‡∏ß‡∏±‡∏î‡∏û‡∏£‡∏∞‡∏ò‡∏≤‡∏ï‡∏∏‡∏ä‡πâ‡∏≤‡∏á‡∏Ñ‡πâ‡∏≥ ‡πÅ‡∏•‡∏∞‡∏ö‡πà‡∏≠‡πÄ‡∏Å‡∏•‡∏∑‡∏≠‡πÉ‡∏ï‡πâ ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô\n",
            "\n",
            "üìö ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á (3 ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£):\n",
            "  1. ‡∏´‡∏ô‡πâ‡∏≤ 2\n",
            "     ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: ‡∏™‡∏°‡∏±‡∏¢‡∏Å‡∏£‡∏∏‡∏á‡∏£‡∏±‡∏ï‡∏ô‡πÇ‡∏Å‡∏™‡∏¥‡∏ô‡∏ó‡∏£‡πå ‡∏ô‡∏Ñ‡∏£‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏ê‡∏≤‡∏ô‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏±‡∏ß‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏£‡∏≤‡∏ä ‡πÄ‡∏à‡πâ‡∏≤‡∏ú‡∏π‡πâ‡∏Ñ‡∏£‡∏≠‡∏á‡∏ô‡∏Ñ‡∏£‡∏ô‡πà‡∏≤‡∏ô‡πÉ‡∏ô‡∏ä‡∏±‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á\n",
            "‡∏ó‡∏∏‡∏Å‡∏≠‡∏á‡∏Ñ‡πå‡∏ï‡πà‡∏≤‡∏á‡∏õ‡∏è‡∏¥‡∏ö...\n",
            "  2. ‡∏´‡∏ô‡πâ‡∏≤ 0\n",
            "     ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: ‡∏Ñ‡∏£‡∏≠‡∏á‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏ó‡∏ô ‡∏à‡∏∂‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏™‡∏ô‡∏≤‡∏≠‡∏≥‡∏°‡∏≤‡∏ï‡∏¢‡πå‡πÑ‡∏õ‡πÄ‡∏ä‡∏¥‡∏ç ‡πÄ‡∏à‡πâ‡∏≤‡πÄ‡∏Å‡πâ‡∏≤‡πÄ‡∏ñ‡∏∑‡πà‡∏≠‡∏ô‡πÄ‡∏Å‡∏£‡∏á‡πÉ‡∏à‡∏õ‡∏π‡πà‡∏à‡∏∂‡∏á‡∏¢‡∏≠‡∏°‡πÑ‡∏õ‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞\n",
            "‡∏°‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏ä‡∏≤‡∏¢‡∏≤‡∏Ñ...\n",
            "  3. ‡∏´‡∏ô‡πâ‡∏≤ 2\n",
            "     ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: ‡∏™‡∏°‡∏±‡∏¢‡∏•‡πâ‡∏≤‡∏ô‡∏ô‡∏≤ \n",
            "‡πÉ‡∏ô‡∏õ‡∏µ ‡∏û.‡∏®. 1993 ‡∏û‡∏£‡∏∞‡πÄ‡∏à‡πâ‡∏≤‡∏ï‡∏¥‡πÇ‡∏•‡∏Å‡∏£‡∏≤‡∏ä‡∏Å‡∏©‡∏±‡∏ï‡∏£‡∏¥‡∏¢‡πå‡∏ô‡∏Ñ‡∏£‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏à‡∏∞‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏≠‡∏á\n",
            "‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÅ...\n",
            "\n",
            "============================================================\n",
            "\n",
            "üìù ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà 3: ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡∏ô‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô ‡πÇ‡∏î‡∏¢‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏±‡∏ß‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏£‡∏≤‡∏ä‡πÉ‡∏ô‡∏™‡∏°‡∏±‡∏¢‡∏Å‡∏£‡∏∏‡∏á‡∏£‡∏±‡∏ï‡∏ô‡πÇ‡∏Å‡∏™‡∏¥‡∏ô‡∏ó‡∏£‡πå ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ñ‡∏¢‡∏ñ‡∏π‡∏Å‡∏ú‡∏ô‡∏ß‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏≠‡∏≤‡∏ì‡∏≤‡∏à‡∏±‡∏Å‡∏£‡∏•‡πâ‡∏≤‡∏ô‡∏ô‡∏≤‡πÉ‡∏ô‡∏™‡∏°‡∏±‡∏¢‡∏û‡∏£‡∏∞‡πÄ‡∏à‡πâ‡∏≤‡∏ï‡∏¥‡πÇ‡∏•‡∏Å‡∏£‡∏≤‡∏ä ‡∏Å‡πà‡∏≠‡∏ô‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏û‡∏°‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏¢‡∏∂‡∏î‡∏Ñ‡∏£‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏õ‡∏µ ‡∏û.‡∏®. 2103‚Äì2328 ‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ã‡∏∂‡∏°‡∏ã‡∏±‡∏ö‡πÄ‡∏≠‡∏≤‡∏®‡∏¥‡∏•‡∏õ‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏Ç‡∏≠‡∏á‡∏•‡πâ‡∏≤‡∏ô‡∏ô‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÉ‡∏ô‡∏ß‡∏¥‡∏ñ‡∏µ‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡πÄ‡∏≠‡∏≤‡∏®‡∏¥‡∏•‡∏õ‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏®‡∏≤‡∏™‡∏ô‡∏≤‡πÅ‡∏ö‡∏ö‡∏•‡πâ‡∏≤‡∏ô‡∏ô‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏®‡∏¥‡∏•‡∏õ‡∏Å‡∏£‡∏£‡∏°‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πÇ‡∏Ç‡∏ó‡∏±‡∏¢\n",
            "\n",
            "üìö ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á (3 ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£):\n",
            "  1. ‡∏´‡∏ô‡πâ‡∏≤ 2\n",
            "     ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: ‡∏™‡∏°‡∏±‡∏¢‡∏Å‡∏£‡∏∏‡∏á‡∏£‡∏±‡∏ï‡∏ô‡πÇ‡∏Å‡∏™‡∏¥‡∏ô‡∏ó‡∏£‡πå ‡∏ô‡∏Ñ‡∏£‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏ê‡∏≤‡∏ô‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏±‡∏ß‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏£‡∏≤‡∏ä ‡πÄ‡∏à‡πâ‡∏≤‡∏ú‡∏π‡πâ‡∏Ñ‡∏£‡∏≠‡∏á‡∏ô‡∏Ñ‡∏£‡∏ô‡πà‡∏≤‡∏ô‡πÉ‡∏ô‡∏ä‡∏±‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á\n",
            "‡∏ó‡∏∏‡∏Å‡∏≠‡∏á‡∏Ñ‡πå‡∏ï‡πà‡∏≤‡∏á‡∏õ‡∏è‡∏¥‡∏ö...\n",
            "  2. ‡∏´‡∏ô‡πâ‡∏≤ 0\n",
            "     ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: ‡∏Ñ‡∏£‡∏≠‡∏á‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏ó‡∏ô ‡∏à‡∏∂‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏™‡∏ô‡∏≤‡∏≠‡∏≥‡∏°‡∏≤‡∏ï‡∏¢‡πå‡πÑ‡∏õ‡πÄ‡∏ä‡∏¥‡∏ç ‡πÄ‡∏à‡πâ‡∏≤‡πÄ‡∏Å‡πâ‡∏≤‡πÄ‡∏ñ‡∏∑‡πà‡∏≠‡∏ô‡πÄ‡∏Å‡∏£‡∏á‡πÉ‡∏à‡∏õ‡∏π‡πà‡∏à‡∏∂‡∏á‡∏¢‡∏≠‡∏°‡πÑ‡∏õ‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞\n",
            "‡∏°‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏ä‡∏≤‡∏¢‡∏≤‡∏Ñ...\n",
            "  3. ‡∏´‡∏ô‡πâ‡∏≤ 2\n",
            "     ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: ‡∏™‡∏°‡∏±‡∏¢‡∏•‡πâ‡∏≤‡∏ô‡∏ô‡∏≤ \n",
            "‡πÉ‡∏ô‡∏õ‡∏µ ‡∏û.‡∏®. 1993 ‡∏û‡∏£‡∏∞‡πÄ‡∏à‡πâ‡∏≤‡∏ï‡∏¥‡πÇ‡∏•‡∏Å‡∏£‡∏≤‡∏ä‡∏Å‡∏©‡∏±‡∏ï‡∏£‡∏¥‡∏¢‡πå‡∏ô‡∏Ñ‡∏£‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏à‡∏∞‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏≠‡∏á\n",
            "‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÅ...\n",
            "\n",
            "============================================================\n",
            "\n",
            "üìù ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà 4: ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏û‡∏∑‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏ñ‡∏∂‡∏á‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏û‡∏∑‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô\n",
            "\n",
            "üìö ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á (3 ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£):\n",
            "  1. ‡∏´‡∏ô‡πâ‡∏≤ 2\n",
            "     ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: ‡∏™‡∏°‡∏±‡∏¢‡∏Å‡∏£‡∏∏‡∏á‡∏£‡∏±‡∏ï‡∏ô‡πÇ‡∏Å‡∏™‡∏¥‡∏ô‡∏ó‡∏£‡πå ‡∏ô‡∏Ñ‡∏£‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏ê‡∏≤‡∏ô‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏±‡∏ß‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏£‡∏≤‡∏ä ‡πÄ‡∏à‡πâ‡∏≤‡∏ú‡∏π‡πâ‡∏Ñ‡∏£‡∏≠‡∏á‡∏ô‡∏Ñ‡∏£‡∏ô‡πà‡∏≤‡∏ô‡πÉ‡∏ô‡∏ä‡∏±‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á\n",
            "‡∏ó‡∏∏‡∏Å‡∏≠‡∏á‡∏Ñ‡πå‡∏ï‡πà‡∏≤‡∏á‡∏õ‡∏è‡∏¥‡∏ö...\n",
            "  2. ‡∏´‡∏ô‡πâ‡∏≤ 0\n",
            "     ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: ‡∏Ñ‡∏£‡∏≠‡∏á‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏ó‡∏ô ‡∏à‡∏∂‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏™‡∏ô‡∏≤‡∏≠‡∏≥‡∏°‡∏≤‡∏ï‡∏¢‡πå‡πÑ‡∏õ‡πÄ‡∏ä‡∏¥‡∏ç ‡πÄ‡∏à‡πâ‡∏≤‡πÄ‡∏Å‡πâ‡∏≤‡πÄ‡∏ñ‡∏∑‡πà‡∏≠‡∏ô‡πÄ‡∏Å‡∏£‡∏á‡πÉ‡∏à‡∏õ‡∏π‡πà‡∏à‡∏∂‡∏á‡∏¢‡∏≠‡∏°‡πÑ‡∏õ‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞\n",
            "‡∏°‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏ä‡∏≤‡∏¢‡∏≤‡∏Ñ...\n",
            "  3. ‡∏´‡∏ô‡πâ‡∏≤ 2\n",
            "     ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: ‡∏™‡∏°‡∏±‡∏¢‡∏•‡πâ‡∏≤‡∏ô‡∏ô‡∏≤ \n",
            "‡πÉ‡∏ô‡∏õ‡∏µ ‡∏û.‡∏®. 1993 ‡∏û‡∏£‡∏∞‡πÄ‡∏à‡πâ‡∏≤‡∏ï‡∏¥‡πÇ‡∏•‡∏Å‡∏£‡∏≤‡∏ä‡∏Å‡∏©‡∏±‡∏ï‡∏£‡∏¥‡∏¢‡πå‡∏ô‡∏Ñ‡∏£‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏à‡∏∞‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏≠‡∏á\n",
            "‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏ô‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÅ...\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö RAG Pipeline\n",
        "def test_rag_pipeline(qa_chain, questions):\n",
        "    \"\"\"‡∏ó‡∏î‡∏™‡∏≠‡∏ö RAG Pipeline\"\"\"\n",
        "    if not qa_chain:\n",
        "        print(\"‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ QA Chain ‡πÉ‡∏´‡πâ‡∏ó‡∏î‡∏™‡∏≠‡∏ö\")\n",
        "        return\n",
        "    \n",
        "    print(\"üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö RAG Pipeline\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for i, question in enumerate(questions, 1):\n",
        "        print(f\"\\nüìù ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà {i}: {question}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        try:\n",
        "            # ‡∏£‡∏±‡∏ô QA Chain\n",
        "            result = qa_chain({\"query\": question})\n",
        "            \n",
        "            # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö\n",
        "            answer = result[\"result\"]\n",
        "            print(f\"ü§ñ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {answer}\")\n",
        "            \n",
        "            # ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á\n",
        "            source_docs = result.get(\"source_documents\", [])\n",
        "            if source_docs:\n",
        "                print(f\"\\nüìö ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á ({len(source_docs)} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£):\")\n",
        "                for j, doc in enumerate(source_docs, 1):\n",
        "                    print(f\"  {j}. ‡∏´‡∏ô‡πâ‡∏≤ {doc.metadata.get('page', 'N/A')}\")\n",
        "                    print(f\"     ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {doc.page_content[:100]}...\")\n",
        "            \n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
        "test_questions = [\n",
        "    \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?\",\n",
        "    \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?\",\n",
        "    \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?\",\n",
        "    \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏û‡∏∑‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?\"\n",
        "]\n",
        "\n",
        "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö RAG Pipeline\n",
        "test_rag_pipeline(qa_chain, test_questions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí¨ ‡∏™‡∏£‡πâ‡∏≤‡∏á ConversationalRetrievalChain\n",
        "\n",
        "### ConversationalRetrievalChain ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n",
        "\n",
        "**ConversationalRetrievalChain** ‡πÄ‡∏õ‡πá‡∏ô chain ‡∏ó‡∏µ‡πà‡∏°‡∏µ memory ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤:\n",
        "1. **Memory** - ‡∏à‡∏î‡∏à‡∏≥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤\n",
        "2. **Retriever** - ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á\n",
        "3. **LLM** - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÇ‡∏î‡∏¢‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤\n",
        "\n",
        "### ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ:\n",
        "- ‚úÖ ‡∏à‡∏î‡∏à‡∏≥‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤\n",
        "- ‚úÖ ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡πÑ‡∏î‡πâ\n",
        "- ‚úÖ ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ ‡∏™‡∏£‡πâ‡∏≤‡∏á Memory ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n",
            "üîç ‡∏™‡∏£‡πâ‡∏≤‡∏á Retriever ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (k=3)\n",
            "‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á ConversationalRetrievalChain ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_40136/2467826205.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(\n"
          ]
        }
      ],
      "source": [
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á ConversationalRetrievalChain\n",
        "def create_conversational_chain(vectorstore, llm, k=3):\n",
        "    \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á ConversationalRetrievalChain\"\"\"\n",
        "    try:\n",
        "        if not vectorstore or not llm:\n",
        "            raise ValueError(\"‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ vectorstore ‡πÅ‡∏•‡∏∞ llm\")\n",
        "        \n",
        "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á memory\n",
        "        memory = ConversationBufferMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            return_messages=True\n",
        "        )\n",
        "        \n",
        "        print(\"üíæ ‡∏™‡∏£‡πâ‡∏≤‡∏á Memory ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
        "        \n",
        "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á retriever\n",
        "        retriever = vectorstore.as_retriever(\n",
        "            search_type=\"similarity\",\n",
        "            search_kwargs={\"k\": k}\n",
        "        )\n",
        "        \n",
        "        print(f\"üîç ‡∏™‡∏£‡πâ‡∏≤‡∏á Retriever ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (k={k})\")\n",
        "        \n",
        "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á ConversationalRetrievalChain\n",
        "        conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=llm,\n",
        "            retriever=retriever,\n",
        "            memory=memory,\n",
        "            return_source_documents=True\n",
        "        )\n",
        "        \n",
        "        print(\"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á ConversationalRetrievalChain ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
        "        \n",
        "        return conversation_chain\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n",
        "        return None\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á ConversationalRetrievalChain\n",
        "if vectorstore and llm:\n",
        "    conversation_chain = create_conversational_chain(vectorstore, llm, k=3)\n",
        "else:\n",
        "    print(\"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á ConversationalRetrievalChain - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö vectorstore ‡πÅ‡∏•‡∏∞ llm\")\n",
        "    conversation_chain = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üó£Ô∏è ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤\n",
        "\n",
        "### ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üó£Ô∏è ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á\n",
            "============================================================\n",
            "\n",
            "üë§ ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?\n",
            "------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: Got multiple output keys: dict_keys(['answer', 'source_documents']), cannot determine which to store in memory. Please set the 'output_key' explicitly.\n",
            "\n",
            "============================================================\n",
            "\n",
            "üë§ ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: ‡πÅ‡∏•‡πâ‡∏ß‡∏°‡∏µ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?\n",
            "------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: Got multiple output keys: dict_keys(['answer', 'source_documents']), cannot determine which to store in memory. Please set the 'output_key' explicitly.\n",
            "\n",
            "============================================================\n",
            "\n",
            "üë§ ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: ‡∏°‡∏µ‡πÄ‡∏Ç‡∏ï‡∏Å‡∏≤‡∏£‡∏õ‡∏Å‡∏Ñ‡∏£‡∏≠‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?\n",
            "------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: Got multiple output keys: dict_keys(['answer', 'source_documents']), cannot determine which to store in memory. Please set the 'output_key' explicitly.\n",
            "\n",
            "============================================================\n",
            "\n",
            "üë§ ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n",
            "------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: Got multiple output keys: dict_keys(['answer', 'source_documents']), cannot determine which to store in memory. Please set the 'output_key' explicitly.\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á\n",
        "def test_conversation(conversation_chain, conversation_flow):\n",
        "    \"\"\"‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á\"\"\"\n",
        "    if not conversation_chain:\n",
        "        print(\"‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ Conversation Chain ‡πÉ‡∏´‡πâ‡∏ó‡∏î‡∏™‡∏≠‡∏ö\")\n",
        "        return\n",
        "    \n",
        "    print(\"üó£Ô∏è ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for i, question in enumerate(conversation_flow, 1):\n",
        "        print(f\"\\nüë§ ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: {question}\")\n",
        "        print(\"-\" * 30)\n",
        "        \n",
        "        try:\n",
        "            # ‡∏£‡∏±‡∏ô conversation chain\n",
        "            result = conversation_chain({\"question\": question})\n",
        "            \n",
        "            # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö\n",
        "            answer = result[\"answer\"]\n",
        "            print(f\"ü§ñ Bot: {answer}\")\n",
        "            \n",
        "            # ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á\n",
        "            source_docs = result.get(\"source_documents\", [])\n",
        "            if source_docs:\n",
        "                print(f\"\\nüìö ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á ({len(source_docs)} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£):\")\n",
        "                for j, doc in enumerate(source_docs, 1):\n",
        "                    print(f\"  {j}. ‡∏´‡∏ô‡πâ‡∏≤ {doc.metadata.get('page', 'N/A')}\")\n",
        "                    print(f\"     ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {doc.page_content[:80]}...\")\n",
        "            \n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á\n",
        "conversation_flow = [\n",
        "    \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?\",\n",
        "    \"‡πÅ‡∏•‡πâ‡∏ß‡∏°‡∏µ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?\",\n",
        "    \"‡∏°‡∏µ‡πÄ‡∏Ç‡∏ï‡∏Å‡∏≤‡∏£‡∏õ‡∏Å‡∏Ñ‡∏£‡∏≠‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?\",\n",
        "    \"‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\"\n",
        "]\n",
        "\n",
        "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤\n",
        "test_conversation(conversation_chain, conversation_flow)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á RAG Pipeline\n",
        "\n",
        "### ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå:\n",
        "\n",
        "1. **k (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤)**\n",
        "   - `k=1`: ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏î‡∏µ‡∏¢‡∏ß, ‡πÄ‡∏£‡πá‡∏ß ‡πÅ‡∏ï‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢\n",
        "   - `k=3`: ‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
        "   - `k=5`: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡∏Å ‡πÅ‡∏ï‡πà‡∏ä‡πâ‡∏≤\n",
        "\n",
        "2. **Temperature**\n",
        "   - `0.0`: ‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö deterministic\n",
        "   - `0.1`: ‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠\n",
        "   - `0.7`: ‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå\n",
        "\n",
        "3. **Chain Type**\n",
        "   - `stuff`: ‡∏£‡∏ß‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô prompt ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
        "   - `map_reduce`: ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡πÜ ‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
        "   - `refine`: ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏≤ k ‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?\n",
            "======================================================================\n",
            "\n",
            "üìä k = 1\n",
            "------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡∏Ç‡∏≠‡∏á‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤...\n",
            "üìö ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á: 1\n",
            "\n",
            "======================================================================\n",
            "\n",
            "üìä k = 3\n",
            "------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡∏Ç‡∏≠‡∏á‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô...\n",
            "üìö ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á: 3\n",
            "\n",
            "======================================================================\n",
            "\n",
            "üìä k = 5\n",
            "------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡∏Ç‡∏≠‡∏á‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤...\n",
            "üìö ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á: 5\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå\n",
        "def test_different_k_values(vectorstore, llm, custom_prompt, question, k_values=[1, 3, 5]):\n",
        "    \"\"\"‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏≤ k ‡∏ï‡πà‡∏≤‡∏á‡πÜ\"\"\"\n",
        "    print(f\"üîß ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏≤ k ‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    for k in k_values:\n",
        "        print(f\"\\nüìä k = {k}\")\n",
        "        print(\"-\" * 30)\n",
        "        \n",
        "        try:\n",
        "            # ‡∏™‡∏£‡πâ‡∏≤‡∏á retriever ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö k ‡∏ô‡∏µ‡πâ\n",
        "            retriever = vectorstore.as_retriever(\n",
        "                search_type=\"similarity\",\n",
        "                search_kwargs={\"k\": k}\n",
        "            )\n",
        "            \n",
        "            # ‡∏™‡∏£‡πâ‡∏≤‡∏á QA chain\n",
        "            qa_chain = RetrievalQA.from_chain_type(\n",
        "                llm=llm,\n",
        "                chain_type=\"stuff\",\n",
        "                retriever=retriever,\n",
        "                chain_type_kwargs={\"prompt\": custom_prompt},\n",
        "                return_source_documents=True\n",
        "            )\n",
        "            \n",
        "            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
        "            result = qa_chain({\"query\": question})\n",
        "            answer = result[\"result\"]\n",
        "            source_docs = result.get(\"source_documents\", [])\n",
        "            \n",
        "            print(f\"ü§ñ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {answer[:200]}...\")\n",
        "            print(f\"üìö ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á: {len(source_docs)}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏≤ k ‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
        "if vectorstore and llm and custom_prompt:\n",
        "    test_question = \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?\"\n",
        "    test_different_k_values(vectorstore, llm, custom_prompt, test_question, [1, 3, 5])\n",
        "else:\n",
        "    print(\"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÑ‡∏î‡πâ - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö vectorstore, llm, ‡πÅ‡∏•‡∏∞ prompt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù ‡∏™‡∏£‡∏∏‡∏õ‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà 3\n",
        "\n",
        "### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
        "\n",
        "1. **RAG Pipeline** üîó\n",
        "   - ‡∏™‡∏£‡πâ‡∏≤‡∏á RetrievalQA Chain\n",
        "   - ‡∏™‡∏£‡πâ‡∏≤‡∏á ConversationalRetrievalChain\n",
        "   - ‡πÉ‡∏ä‡πâ Custom Prompt Template\n",
        "\n",
        "2. **LLM Integration** ü§ñ\n",
        "   - ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Groq LLM\n",
        "   - ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Vector Store\n",
        "   - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°\n",
        "\n",
        "3. **Memory & Conversation** üí¨\n",
        "   - ‡πÉ‡∏ä‡πâ ConversationBufferMemory\n",
        "   - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á\n",
        "   - ‡∏à‡∏î‡∏à‡∏≥‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤\n",
        "\n",
        "4. **Parameter Tuning** üîß\n",
        "   - ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏Ñ‡πà‡∏≤ k (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£)\n",
        "   - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
        "   - ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\n",
        "\n",
        "### Pipeline ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå:\n",
        "```\n",
        "Question ‚Üí Embeddings ‚Üí Vector Search ‚Üí Retriever ‚Üí Prompt ‚Üí LLM ‚Üí Answer\n",
        "```\n",
        "\n",
        "### ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà 4:\n",
        "‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Streamlit UI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RAG Chatbot\n",
        "\n",
        "---\n",
        "\n",
        "**‚è∞ ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: 60 ‡∏ô‡∏≤‡∏ó‡∏µ**\n",
        "**üìö ‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πà‡∏≠‡πÑ‡∏õ: `04_streamlit_ui.ipynb`**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
