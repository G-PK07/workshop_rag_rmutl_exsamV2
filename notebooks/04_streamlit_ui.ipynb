{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìö Workshop: RAG + LangChain + Streamlit\n",
        "\n",
        "## ‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà 4: Deploy with Streamlit UI (2:30 ‚Äì 3:30)\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ\n",
        "- ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å Streamlit framework\n",
        "- ‡∏™‡∏£‡πâ‡∏≤‡∏á UI chatbot\n",
        "- ‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°\n",
        "- ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å RAG\n",
        "- ‡πÄ‡∏Å‡πá‡∏ö history ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤\n",
        "- ‡πÅ‡∏™‡∏î‡∏á context ‡∏ó‡∏µ‡πà RAG ‡∏î‡∏∂‡∏á‡∏°‡∏≤\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üåê Streamlit Framework\n",
        "\n",
        "### Streamlit ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n",
        "\n",
        "**Streamlit** ‡πÄ‡∏õ‡πá‡∏ô Python framework ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á web applications ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡πá‡∏ß\n",
        "\n",
        "### ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á Streamlit:\n",
        "\n",
        "1. **‡∏á‡πà‡∏≤‡∏¢‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô** üöÄ\n",
        "   - ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÅ‡∏Ñ‡πà Python code\n",
        "   - ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ HTML/CSS/JavaScript\n",
        "\n",
        "2. **‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤** ‚ö°\n",
        "   - ‡∏™‡∏£‡πâ‡∏≤‡∏á UI ‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡πÑ‡∏°‡πà‡∏Å‡∏µ‡πà‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î\n",
        "   - Auto-refresh ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÇ‡∏Ñ‡πâ‡∏î\n",
        "\n",
        "3. **‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Data Science** üìä\n",
        "   - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
        "   - ‡∏°‡∏µ widgets ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö input/output\n",
        "\n",
        "4. **Deploy ‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢** üåê\n",
        "   - Deploy ‡∏ö‡∏ô Streamlit Cloud\n",
        "   - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Docker\n",
        "\n",
        "### Streamlit Components ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ:\n",
        "- `st.title()` - ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠\n",
        "- `st.text_input()` - ‡∏ä‡πà‡∏≠‡∏á‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\n",
        "- `st.button()` - ‡∏õ‡∏∏‡πà‡∏°\n",
        "- `st.chat_message()` - ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ä‡∏ó\n",
        "- `st.session_state` - ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/game/01_Projects/workshop_rag_rmutl/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Streamlit ‡πÅ‡∏•‡∏∞ LangChain libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Streamlit\n",
        "import streamlit as st\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# ‡πÄ‡∏û‡∏¥‡πà‡∏° path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö import modules\n",
        "sys.path.append('..')\n",
        "\n",
        "# Import LangChain components\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Import utilities\n",
        "from dotenv import load_dotenv\n",
        "import logging\n",
        "\n",
        "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ‡πÇ‡∏´‡∏•‡∏î environment variables\n",
        "load_dotenv()\n",
        "\n",
        "print(\"‚úÖ Streamlit ‡πÅ‡∏•‡∏∞ LangChain libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèóÔ∏è ‡∏™‡∏£‡πâ‡∏≤‡∏á RAG Service Class\n",
        "\n",
        "### RAG Service Class\n",
        "\n",
        "‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á class ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏° RAG functionality ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:\n",
        "\n",
        "1. **‡πÇ‡∏´‡∏•‡∏î Vector Store**\n",
        "2. **‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LLM**\n",
        "3. **‡∏™‡∏£‡πâ‡∏≤‡∏á Chains**\n",
        "4. **‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ RAG Service Class ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n"
          ]
        }
      ],
      "source": [
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á RAG Service Class\n",
        "class RAGService:\n",
        "    \"\"\"RAG Service ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Streamlit\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.vectorstore = None\n",
        "        self.llm = None\n",
        "        self.qa_chain = None\n",
        "        self.conversation_chain = None\n",
        "        self.embeddings = None\n",
        "        \n",
        "    def load_vectorstore(self, vectorstore_path=\"../vectorstore\"):\n",
        "        \"\"\"‡πÇ‡∏´‡∏•‡∏î Vector Store\"\"\"\n",
        "        try:\n",
        "            # ‡πÇ‡∏´‡∏•‡∏î embeddings\n",
        "            self.embeddings = HuggingFaceEmbeddings(\n",
        "                model_name=\"all-MiniLM-L6-v2\",\n",
        "                cache_folder=\"../model_cache\"\n",
        "            )\n",
        "            \n",
        "            # ‡πÇ‡∏´‡∏•‡∏î FAISS vector store\n",
        "            self.vectorstore = FAISS.load_local(\n",
        "                vectorstore_path, \n",
        "                self.embeddings, \n",
        "                allow_dangerous_deserialization=True\n",
        "            )\n",
        "            \n",
        "            print(f\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î Vector Store ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {self.vectorstore.index.ntotal} documents\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î Vector Store: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def setup_llm(self, model_name=\"llama-3.3-70b-versatile\", temperature=0.1):\n",
        "        \"\"\"‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LLM\"\"\"\n",
        "        try:\n",
        "            api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "            if not api_key:\n",
        "                raise ValueError(\"GROQ_API_KEY ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ô environment variables\")\n",
        "            \n",
        "            self.llm = ChatGroq(\n",
        "                groq_api_key=api_key,\n",
        "                model_name=model_name,\n",
        "                temperature=temperature\n",
        "            )\n",
        "            \n",
        "            print(f\"‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Groq LLM ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {model_name}\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def create_custom_prompt(self):\n",
        "        \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Prompt Template\"\"\"\n",
        "        prompt_template = \"\"\"\n",
        "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡∏à‡∏á‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\n",
        "\n",
        "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:\n",
        "{context}\n",
        "\n",
        "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}\n",
        "\n",
        "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\"\"\"\n",
        "        \n",
        "        return PromptTemplate(\n",
        "            template=prompt_template,\n",
        "            input_variables=[\"context\", \"question\"]\n",
        "        )\n",
        "    \n",
        "    def setup_qa_chain(self, k=3):\n",
        "        \"\"\"‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ QA Chain\"\"\"\n",
        "        try:\n",
        "            if not self.vectorstore or not self.llm:\n",
        "                raise ValueError(\"‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ vectorstore ‡πÅ‡∏•‡∏∞ llm\")\n",
        "            \n",
        "            # ‡∏™‡∏£‡πâ‡∏≤‡∏á retriever\n",
        "            retriever = self.vectorstore.as_retriever(\n",
        "                search_type=\"similarity\",\n",
        "                search_kwargs={\"k\": k}\n",
        "            )\n",
        "            \n",
        "            # ‡∏™‡∏£‡πâ‡∏≤‡∏á custom prompt\n",
        "            custom_prompt = self.create_custom_prompt()\n",
        "            \n",
        "            # ‡∏™‡∏£‡πâ‡∏≤‡∏á QA chain\n",
        "            self.qa_chain = RetrievalQA.from_chain_type(\n",
        "                llm=self.llm,\n",
        "                chain_type=\"stuff\",\n",
        "                retriever=retriever,\n",
        "                chain_type_kwargs={\"prompt\": custom_prompt},\n",
        "                return_source_documents=True\n",
        "            )\n",
        "            \n",
        "            print(\"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á QA Chain ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def setup_conversation_chain(self, k=3):\n",
        "        \"\"\"‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Conversation Chain\"\"\"\n",
        "        try:\n",
        "            if not self.vectorstore or not self.llm:\n",
        "                raise ValueError(\"‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ vectorstore ‡πÅ‡∏•‡∏∞ llm\")\n",
        "            \n",
        "            # ‡∏™‡∏£‡πâ‡∏≤‡∏á memory\n",
        "            memory = ConversationBufferMemory(\n",
        "                memory_key=\"chat_history\",\n",
        "                return_messages=True\n",
        "            )\n",
        "            \n",
        "            # ‡∏™‡∏£‡πâ‡∏≤‡∏á retriever\n",
        "            retriever = self.vectorstore.as_retriever(\n",
        "                search_type=\"similarity\",\n",
        "                search_kwargs={\"k\": k}\n",
        "            )\n",
        "            \n",
        "            # ‡∏™‡∏£‡πâ‡∏≤‡∏á conversation chain\n",
        "            self.conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "                llm=self.llm,\n",
        "                retriever=retriever,\n",
        "                memory=memory,\n",
        "                return_source_documents=True\n",
        "            )\n",
        "            \n",
        "            print(\"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Conversation Chain ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def answer_question(self, question, use_conversation=False):\n",
        "        \"\"\"‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°\"\"\"\n",
        "        try:\n",
        "            if use_conversation and self.conversation_chain:\n",
        "                result = self.conversation_chain({\"question\": question})\n",
        "                answer = result[\"answer\"]\n",
        "                source_docs = result.get(\"source_documents\", [])\n",
        "            elif self.qa_chain:\n",
        "                result = self.qa_chain({\"query\": question})\n",
        "                answer = result[\"result\"]\n",
        "                source_docs = result.get(\"source_documents\", [])\n",
        "            else:\n",
        "                raise ValueError(\"‡πÑ‡∏°‡πà‡∏°‡∏µ chain ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\")\n",
        "            \n",
        "            # ‡πÅ‡∏õ‡∏•‡∏á source documents\n",
        "            sources = []\n",
        "            for doc in source_docs:\n",
        "                sources.append({\n",
        "                    \"content\": doc.page_content,\n",
        "                    \"metadata\": doc.metadata\n",
        "                })\n",
        "            \n",
        "            return {\n",
        "                \"answer\": answer,\n",
        "                \"sources\": sources,\n",
        "                \"method\": \"conversation\" if use_conversation else \"qa\"\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"answer\": f\"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\",\n",
        "                \"sources\": [],\n",
        "                \"method\": \"error\"\n",
        "            }\n",
        "    \n",
        "    def initialize(self):\n",
        "        \"\"\"‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\"\"\"\n",
        "        print(\"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô RAG Service...\")\n",
        "        \n",
        "        # ‡πÇ‡∏´‡∏•‡∏î vectorstore\n",
        "        if not self.load_vectorstore():\n",
        "            return False\n",
        "        \n",
        "        # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LLM\n",
        "        if not self.setup_llm():\n",
        "            return False\n",
        "        \n",
        "        # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ chains\n",
        "        if not self.setup_qa_chain():\n",
        "            return False\n",
        "        \n",
        "        if not self.setup_conversation_chain():\n",
        "            return False\n",
        "        \n",
        "        print(\"‚úÖ RAG Service ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
        "        return True\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á RAG Service instance\n",
        "rag_service = RAGService()\n",
        "print(\"üì¶ RAG Service Class ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé® ‡∏™‡∏£‡πâ‡∏≤‡∏á Streamlit UI\n",
        "\n",
        "### Streamlit App Structure:\n",
        "\n",
        "1. **Page Configuration** - ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤\n",
        "2. **Sidebar** - ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤\n",
        "3. **Main Area** - Chat interface\n",
        "4. **Chat History** - ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤\n",
        "5. **Input Area** - ‡∏ä‡πà‡∏≠‡∏á‡∏Å‡∏£‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üé® Streamlit App function ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n",
            "üí° ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á: streamlit run 04_streamlit_ui.py\n"
          ]
        }
      ],
      "source": [
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Streamlit App\n",
        "def create_streamlit_app():\n",
        "    \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á Streamlit App\"\"\"\n",
        "    \n",
        "    # Page configuration\n",
        "    st.set_page_config(\n",
        "        page_title=\"RAG Chatbot with LangChain\",\n",
        "        page_icon=\"ü§ñ\",\n",
        "        layout=\"wide\",\n",
        "        initial_sidebar_state=\"expanded\"\n",
        "    )\n",
        "    \n",
        "    # CSS styling\n",
        "    st.markdown(\"\"\"\n",
        "    <style>\n",
        "    .main-header {\n",
        "        font-size: 2.5rem;\n",
        "        color: #1f77b4;\n",
        "        text-align: center;\n",
        "        margin-bottom: 2rem;\n",
        "    }\n",
        "    .chat-message {\n",
        "        padding: 1rem;\n",
        "        border-radius: 0.5rem;\n",
        "        margin: 0.5rem 0;\n",
        "    }\n",
        "    .user-message {\n",
        "        background-color: #e3f2fd;\n",
        "        border-left: 4px solid #2196f3;\n",
        "    }\n",
        "    .assistant-message {\n",
        "        background-color: #f3e5f5;\n",
        "        border-left: 4px solid #9c27b0;\n",
        "    }\n",
        "    .source-info {\n",
        "        background-color: #f5f5f5;\n",
        "        padding: 0.5rem;\n",
        "        border-radius: 0.25rem;\n",
        "        margin: 0.5rem 0;\n",
        "        font-size: 0.9rem;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "    \n",
        "    # Main header\n",
        "    st.markdown('<h1 class=\"main-header\">ü§ñ RAG Chatbot with LangChain + Streamlit</h1>', unsafe_allow_html=True)\n",
        "    \n",
        "    # Sidebar\n",
        "    with st.sidebar:\n",
        "        st.header(\"‚öôÔ∏è ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤\")\n",
        "        \n",
        "        # RAG Mode\n",
        "        rag_mode = st.selectbox(\n",
        "            \"‡πÇ‡∏´‡∏°‡∏î RAG\",\n",
        "            [\"Simple QA\", \"Conversational\"],\n",
        "            help=\"Simple QA: ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πâ‡∏≠, Conversational: ‡∏à‡∏î‡∏à‡∏≥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤\"\n",
        "        )\n",
        "        \n",
        "        # K value\n",
        "        k_value = st.slider(\n",
        "            \"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ (k)\",\n",
        "            min_value=1,\n",
        "            max_value=10,\n",
        "            value=3,\n",
        "            help=\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°\"\n",
        "        )\n",
        "        \n",
        "        # Temperature\n",
        "        temperature = st.slider(\n",
        "            \"Temperature\",\n",
        "            min_value=0.0,\n",
        "            max_value=1.0,\n",
        "            value=0.1,\n",
        "            step=0.1,\n",
        "            help=\"‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö\"\n",
        "        )\n",
        "        \n",
        "        # Clear chat button\n",
        "        if st.button(\"üóëÔ∏è ‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤\", use_container_width=True):\n",
        "            st.session_state.messages = []\n",
        "            st.rerun()\n",
        "    \n",
        "    # Initialize RAG Service\n",
        "    @st.cache_resource\n",
        "    def get_rag_service():\n",
        "        \"\"\"Get cached RAG service\"\"\"\n",
        "        service = RAGService()\n",
        "        if service.initialize():\n",
        "            return service\n",
        "        return None\n",
        "    \n",
        "    rag_service = get_rag_service()\n",
        "    \n",
        "    if rag_service is None:\n",
        "        st.error(\"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô RAG Service ‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤\")\n",
        "        st.info(\"üí° ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤:\")\n",
        "        st.info(\"- ‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå .env ‡∏û‡∏£‡πâ‡∏≠‡∏° GROQ_API_KEY\")\n",
        "        st.info(\"- ‡∏°‡∏µ Vector Store ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå ../vectorstore\")\n",
        "        return\n",
        "    \n",
        "    # Initialize chat history\n",
        "    if \"messages\" not in st.session_state:\n",
        "        st.session_state.messages = []\n",
        "    \n",
        "    # Display chat history\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.markdown(message[\"content\"])\n",
        "            \n",
        "            # Show sources if available\n",
        "            if message.get(\"sources\"):\n",
        "                with st.expander(\"üìö ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á\"):\n",
        "                    for i, source in enumerate(message[\"sources\"], 1):\n",
        "                        st.markdown(f\"**‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà {i}:**\")\n",
        "                        st.markdown(f\"‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {source['content'][:200]}...\")\n",
        "                        if source.get('metadata'):\n",
        "                            st.markdown(f\"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°: {source['metadata']}\")\n",
        "                        st.markdown(\"---\")\n",
        "    \n",
        "    # Chat input\n",
        "    if prompt := st.chat_input(\"‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì...\"):\n",
        "        # Add user message to chat history\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        \n",
        "        # Display user message\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(prompt)\n",
        "        \n",
        "        # Generate and display assistant response\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"ü§î ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏î‡∏î‡πâ‡∏ß‡∏¢ RAG...\"):\n",
        "                try:\n",
        "                    use_conversation = rag_mode == \"Conversational\"\n",
        "                    response = rag_service.answer_question(prompt, use_conversation=use_conversation)\n",
        "                    \n",
        "                    # Display answer\n",
        "                    st.markdown(response[\"answer\"])\n",
        "                    \n",
        "                    # Show sources\n",
        "                    if response.get(\"sources\"):\n",
        "                        with st.expander(\"üìö ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á\"):\n",
        "                            for i, source in enumerate(response[\"sources\"], 1):\n",
        "                                st.markdown(f\"**‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà {i}:**\")\n",
        "                                st.markdown(f\"‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {source['content'][:200]}...\")\n",
        "                                if source.get('metadata'):\n",
        "                                    st.markdown(f\"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°: {source['metadata']}\")\n",
        "                                st.markdown(\"---\")\n",
        "                    \n",
        "                    # Add assistant response to chat history\n",
        "                    st.session_state.messages.append({\n",
        "                        \"role\": \"assistant\", \n",
        "                        \"content\": response[\"answer\"],\n",
        "                        \"sources\": response.get(\"sources\", []),\n",
        "                        \"method\": response.get(\"method\", \"unknown\")\n",
        "                    })\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    error_msg = f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\"\n",
        "                    st.error(error_msg)\n",
        "                    st.session_state.messages.append({\"role\": \"assistant\", \"content\": error_msg})\n",
        "    \n",
        "    # Quick questions\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"‚ùì ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\")\n",
        "    \n",
        "    # Categorized quick questions\n",
        "    categories = {\n",
        "        \"üèõÔ∏è ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå\": [\n",
        "            \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?\",\n",
        "            \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á‡∏≠‡∏≤‡∏ì‡∏≤‡∏à‡∏±‡∏Å‡∏£‡πÉ‡∏î‡∏ö‡πâ‡∏≤‡∏á?\"\n",
        "        ],\n",
        "        \"üèûÔ∏è ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß\": [\n",
        "            \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?\",\n",
        "            \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏ß‡∏±‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?\"\n",
        "        ],\n",
        "        \"üçΩÔ∏è ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°\": [\n",
        "            \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏û‡∏∑‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?\",\n",
        "            \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏≠‡∏∞‡πÑ‡∏£‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à?\"\n",
        "        ],\n",
        "        \"üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ\": [\n",
        "            \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?\",\n",
        "            \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?\"\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    for category, questions in categories.items():\n",
        "        with st.expander(category):\n",
        "            cols = st.columns(2)\n",
        "            for i, question in enumerate(questions):\n",
        "                with cols[i % 2]:\n",
        "                    if st.button(question, use_container_width=True, key=f\"quick_{category}_{i}\"):\n",
        "                        # Add user message to chat history\n",
        "                        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
        "                        \n",
        "                        # Generate and add assistant response\n",
        "                        with st.spinner(\"ü§î ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏î...\"):\n",
        "                            try:\n",
        "                                use_conversation = rag_mode == \"Conversational\"\n",
        "                                response = rag_service.answer_question(question, use_conversation=use_conversation)\n",
        "                                \n",
        "                                # Add assistant response to chat history\n",
        "                                st.session_state.messages.append({\n",
        "                                    \"role\": \"assistant\", \n",
        "                                    \"content\": response[\"answer\"],\n",
        "                                    \"sources\": response.get(\"sources\", []),\n",
        "                                    \"method\": response.get(\"method\", \"unknown\")\n",
        "                                })\n",
        "                                \n",
        "                            except Exception as e:\n",
        "                                error_msg = f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\"\n",
        "                                st.session_state.messages.append({\"role\": \"assistant\", \"content\": error_msg})\n",
        "                        \n",
        "                        st.rerun()\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Streamlit App\n",
        "print(\"üé® Streamlit App function ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
        "print(\"üí° ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á: streamlit run 04_streamlit_ui.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÑ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Streamlit App\n",
        "\n",
        "### ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `streamlit_app.py`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå streamlit_app.py ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n",
            "üí° ‡∏ß‡∏¥‡∏ò‡∏µ‡∏£‡∏±‡∏ô: streamlit run streamlit_app.py\n"
          ]
        }
      ],
      "source": [
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå streamlit_app.py\n",
        "streamlit_app_code = '''# streamlit_app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# ‡πÄ‡∏û‡∏¥‡πà‡∏° path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö import modules\n",
        "sys.path.append('.')\n",
        "\n",
        "# Import LangChain components\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Import utilities\n",
        "from dotenv import load_dotenv\n",
        "import logging\n",
        "\n",
        "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ‡πÇ‡∏´‡∏•‡∏î environment variables\n",
        "load_dotenv()\n",
        "\n",
        "class RAGService:\n",
        "    \"\"\"RAG Service ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Streamlit\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.vectorstore = None\n",
        "        self.llm = None\n",
        "        self.qa_chain = None\n",
        "        self.embeddings = None\n",
        "\n",
        "    def load_vectorstore(self, vectorstore_path=\"./vectorstore\"):\n",
        "        \"\"\"‡πÇ‡∏´‡∏•‡∏î Vector Store\"\"\"\n",
        "        try:\n",
        "            # ‡πÇ‡∏´‡∏•‡∏î embeddings\n",
        "            self.embeddings = HuggingFaceEmbeddings(\n",
        "                model_name=\"all-MiniLM-L6-v2\",\n",
        "                cache_folder=\"./model_cache\"\n",
        "            )\n",
        "\n",
        "            # ‡πÇ‡∏´‡∏•‡∏î FAISS vector store\n",
        "            self.vectorstore = FAISS.load_local(\n",
        "                vectorstore_path, \n",
        "                self.embeddings, \n",
        "                allow_dangerous_deserialization=True\n",
        "            )\n",
        "\n",
        "            logger.info(f\"‡πÇ‡∏´‡∏•‡∏î Vector Store ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {self.vectorstore.index.ntotal} documents\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î Vector Store: {e}\")\n",
        "            return False\n",
        "\n",
        "    def setup_llm(self, model_name=\"llama-3.3-70b-versatile\", temperature=0.1):\n",
        "        \"\"\"‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LLM\"\"\"\n",
        "        try:\n",
        "            api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "            if not api_key:\n",
        "                raise ValueError(\"GROQ_API_KEY ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ô environment variables\")\n",
        "\n",
        "            self.llm = ChatGroq(\n",
        "                groq_api_key=api_key,\n",
        "                model_name=model_name,\n",
        "                temperature=temperature\n",
        "            )\n",
        "\n",
        "            logger.info(f\"‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Groq LLM ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {model_name}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n",
        "            return False\n",
        "\n",
        "    def create_custom_prompt(self):\n",
        "        \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Prompt Template\"\"\"\n",
        "        prompt_template = \"\"\"\n",
        "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡∏à‡∏á‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\n",
        "\n",
        "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:\n",
        "{context}\n",
        "\n",
        "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}\n",
        "\n",
        "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\"\"\"\n",
        "\n",
        "        return PromptTemplate(\n",
        "            template=prompt_template,\n",
        "            input_variables=[\"context\", \"question\"]\n",
        "        )\n",
        "\n",
        "    def setup_qa_chain(self, k=3):\n",
        "        \"\"\"‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ QA Chain\"\"\"\n",
        "        try:\n",
        "            if not self.vectorstore or not self.llm:\n",
        "                raise ValueError(\"‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ vectorstore ‡πÅ‡∏•‡∏∞ llm\")\n",
        "\n",
        "            # ‡∏™‡∏£‡πâ‡∏≤‡∏á retriever\n",
        "            retriever = self.vectorstore.as_retriever(\n",
        "                search_type=\"similarity\",\n",
        "                search_kwargs={\"k\": k}\n",
        "            )\n",
        "\n",
        "            # ‡∏™‡∏£‡πâ‡∏≤‡∏á custom prompt\n",
        "            custom_prompt = self.create_custom_prompt()\n",
        "\n",
        "            # ‡∏™‡∏£‡πâ‡∏≤‡∏á QA chain\n",
        "            self.qa_chain = RetrievalQA.from_chain_type(\n",
        "                llm=self.llm,\n",
        "                chain_type=\"stuff\",\n",
        "                retriever=retriever,\n",
        "                chain_type_kwargs={\"prompt\": custom_prompt},\n",
        "                return_source_documents=True\n",
        "            )\n",
        "\n",
        "            logger.info(\"‡∏™‡∏£‡πâ‡∏≤‡∏á QA Chain ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n",
        "            return False\n",
        "\n",
        "    def answer_question(self, question):\n",
        "        \"\"\"‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°\"\"\"\n",
        "        try:\n",
        "            if not self.qa_chain:\n",
        "                raise ValueError(\"‡πÑ‡∏°‡πà‡∏°‡∏µ QA chain ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\")\n",
        "\n",
        "            result = self.qa_chain({\"query\": question})\n",
        "            answer = result[\"result\"]\n",
        "            source_docs = result.get(\"source_documents\", [])\n",
        "\n",
        "            # ‡πÅ‡∏õ‡∏•‡∏á source documents\n",
        "            sources = []\n",
        "            for doc in source_docs:\n",
        "                sources.append({\n",
        "                    \"content\": doc.page_content,\n",
        "                    \"metadata\": doc.metadata\n",
        "                })\n",
        "\n",
        "            return {\n",
        "                \"answer\": answer,\n",
        "                \"sources\": sources,\n",
        "                \"method\": \"qa\"\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"answer\": f\"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\",\n",
        "                \"sources\": [],\n",
        "                \"method\": \"error\"\n",
        "            }\n",
        "\n",
        "    def initialize(self):\n",
        "        \"\"\"‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\"\"\"\n",
        "        logger.info(\"‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô RAG Service...\")\n",
        "\n",
        "        # ‡πÇ‡∏´‡∏•‡∏î vectorstore\n",
        "        if not self.load_vectorstore():\n",
        "            return False\n",
        "\n",
        "        # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LLM\n",
        "        if not self.setup_llm():\n",
        "            return False\n",
        "\n",
        "        # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ chains\n",
        "        if not self.setup_qa_chain():\n",
        "            return False\n",
        "\n",
        "        logger.info(\"RAG Service ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
        "        return True\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main Streamlit application\"\"\"\n",
        "\n",
        "    # Page configuration\n",
        "    st.set_page_config(\n",
        "        page_title=\"RAG Chatbot with LangChain\",\n",
        "        page_icon=\"ü§ñ\",\n",
        "        layout=\"wide\",\n",
        "        initial_sidebar_state=\"expanded\"\n",
        "    )\n",
        "\n",
        "    # CSS styling\n",
        "    st.markdown(\"\"\"\n",
        "    <style>\n",
        "    .main-header {\n",
        "        font-size: 2.5rem;\n",
        "        color: #1f77b4;\n",
        "        text-align: center;\n",
        "        margin-bottom: 2rem;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    # Main header\n",
        "    st.markdown('<h1 class=\"main-header\">ü§ñ RAG Chatbot with LangChain + Streamlit</h1>', unsafe_allow_html=True)\n",
        "\n",
        "    # Sidebar\n",
        "    with st.sidebar:\n",
        "        st.header(\"‚öôÔ∏è ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤\")\n",
        "\n",
        "        # Clear chat button\n",
        "        if st.button(\"üóëÔ∏è ‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤\", use_container_width=True):\n",
        "            st.session_state.messages = []\n",
        "            st.rerun()\n",
        "\n",
        "    # Initialize RAG Service\n",
        "    @st.cache_resource\n",
        "    def get_rag_service():\n",
        "        \"\"\"Get cached RAG service\"\"\"\n",
        "        service = RAGService()\n",
        "        if service.initialize():\n",
        "            return service\n",
        "        return None\n",
        "\n",
        "    rag_service = get_rag_service()\n",
        "\n",
        "    if rag_service is None:\n",
        "        st.error(\"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô RAG Service ‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤\")\n",
        "        st.info(\"üí° ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤:\")\n",
        "        st.info(\"- ‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå .env ‡∏û‡∏£‡πâ‡∏≠‡∏° GROQ_API_KEY\")\n",
        "        st.info(\"- ‡∏°‡∏µ Vector Store ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå ./vectorstore\")\n",
        "        return\n",
        "\n",
        "    # Initialize chat history\n",
        "    if \"messages\" not in st.session_state:\n",
        "        st.session_state.messages = []\n",
        "\n",
        "    # Display chat history\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.markdown(message[\"content\"])\n",
        "\n",
        "            # Show sources if available\n",
        "            if message.get(\"sources\"):\n",
        "                with st.expander(\"üìö ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á\"):\n",
        "                    for i, source in enumerate(message[\"sources\"], 1):\n",
        "                        st.markdown(f\"**‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà {i}:**\")\n",
        "                        st.markdown(f\"‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {source['content'][:200]}...\")\n",
        "                        if source.get('metadata'):\n",
        "                            st.markdown(f\"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°: {source['metadata']}\")\n",
        "                        st.markdown(\"---\")\n",
        "\n",
        "    # Chat input\n",
        "    if prompt := st.chat_input(\"‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì...\"):\n",
        "        # Add user message to chat history\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "        # Display user message\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(prompt)\n",
        "\n",
        "        # Generate and display assistant response\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"ü§î ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏î‡∏î‡πâ‡∏ß‡∏¢ RAG...\"):\n",
        "                try:\n",
        "                    response = rag_service.answer_question(prompt)\n",
        "\n",
        "                    # Display answer\n",
        "                    st.markdown(response[\"answer\"])\n",
        "\n",
        "                    # Show sources\n",
        "                    if response.get(\"sources\"):\n",
        "                        with st.expander(\"üìö ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á\"):\n",
        "                            for i, source in enumerate(response[\"sources\"], 1):\n",
        "                                st.markdown(f\"**‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà {i}:**\")\n",
        "                                st.markdown(f\"‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {source['content'][:200]}...\")\n",
        "                                if source.get('metadata'):\n",
        "                                    st.markdown(f\"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°: {source['metadata']}\")\n",
        "                                st.markdown(\"---\")\n",
        "\n",
        "                    # Add assistant response to chat history\n",
        "                    st.session_state.messages.append({\n",
        "                        \"role\": \"assistant\", \n",
        "                        \"content\": response[\"answer\"],\n",
        "                        \"sources\": response.get(\"sources\", []),\n",
        "                        \"method\": response.get(\"method\", \"unknown\")\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    error_msg = f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\"\n",
        "                    st.error(error_msg)\n",
        "                    st.session_state.messages.append({\"role\": \"assistant\", \"content\": error_msg})\n",
        "\n",
        "    # Quick questions\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"‚ùì ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\")\n",
        "\n",
        "    # Categorized quick questions\n",
        "    categories = {\n",
        "        \"üèõÔ∏è ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå\": [\n",
        "            \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?\",\n",
        "            \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á‡∏≠‡∏≤‡∏ì‡∏≤‡∏à‡∏±‡∏Å‡∏£‡πÉ‡∏î‡∏ö‡πâ‡∏≤‡∏á?\"\n",
        "        ],\n",
        "        \"üèûÔ∏è ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß\": [\n",
        "            \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?\",\n",
        "            \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏ß‡∏±‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?\"\n",
        "        ],\n",
        "        \"üçΩÔ∏è ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°\": [\n",
        "            \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏û‡∏∑‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?\",\n",
        "            \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏≠‡∏∞‡πÑ‡∏£‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à?\"\n",
        "        ],\n",
        "        \"üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ\": [\n",
        "            \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?\",\n",
        "            \"‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô‡∏°‡∏µ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    for category, questions in categories.items():\n",
        "        with st.expander(category):\n",
        "            cols = st.columns(2)\n",
        "            for i, question in enumerate(questions):\n",
        "                with cols[i % 2]:\n",
        "                    if st.button(question, use_container_width=True, key=f\"quick_{category}_{i}\"):\n",
        "                        # Add user message to chat history\n",
        "                        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
        "\n",
        "                        # Generate and add assistant response\n",
        "                        with st.spinner(\"ü§î ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏î...\"):\n",
        "                            try:\n",
        "                                response = rag_service.answer_question(question)\n",
        "\n",
        "                                # Add assistant response to chat history\n",
        "                                st.session_state.messages.append({\n",
        "                                    \"role\": \"assistant\", \n",
        "                                    \"content\": response[\"answer\"],\n",
        "                                    \"sources\": response.get(\"sources\", []),\n",
        "                                    \"method\": response.get(\"method\", \"unknown\")\n",
        "                                })\n",
        "\n",
        "                            except Exception as e:\n",
        "                                error_msg = f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\"\n",
        "                                st.session_state.messages.append({\"role\": \"assistant\", \"content\": error_msg})\n",
        "\n",
        "                        st.rerun()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "'''\n",
        "\n",
        "# ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå streamlit_app.py\n",
        "with open(\"../streamlit_app.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(streamlit_app_code)\n",
        "\n",
        "print(\"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå streamlit_app.py ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
        "print(\"üí° ‡∏ß‡∏¥‡∏ò‡∏µ‡∏£‡∏±‡∏ô: streamlit run streamlit_app.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏£‡∏±‡∏ô Streamlit App\n",
        "\n",
        "### ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ô:\n",
        "\n",
        "```bash\n",
        "# ‡∏£‡∏±‡∏ô Streamlit App\n",
        "streamlit run streamlit_app.py\n",
        "\n",
        "# ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ uv (‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)\n",
        "uv run streamlit run streamlit_app.py\n",
        "```\n",
        "\n",
        "### ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á Streamlit App:\n",
        "\n",
        "1. **Chat Interface** üí¨\n",
        "   - ‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°\n",
        "   - ‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤\n",
        "   - ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á\n",
        "\n",
        "2. **Sidebar Controls** ‚öôÔ∏è\n",
        "   - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î RAG (Simple QA / Conversational)\n",
        "   - ‡∏õ‡∏∏‡πà‡∏°‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤\n",
        "\n",
        "3. **Quick Questions** ‚ùì\n",
        "   - ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏°‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà\n",
        "   - ‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ\n",
        "\n",
        "4. **Source Citations** üìö\n",
        "   - ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á\n",
        "   - ‡πÅ‡∏™‡∏î‡∏á metadata ‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù ‡∏™‡∏£‡∏∏‡∏õ‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà 4\n",
        "\n",
        "### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
        "\n",
        "1. **Streamlit Framework** üåê\n",
        "   - ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å Streamlit ‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏î‡∏µ\n",
        "   - Components ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á UI\n",
        "   - ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ page configuration\n",
        "\n",
        "2. **RAG Service Class** üèóÔ∏è\n",
        "   - ‡∏™‡∏£‡πâ‡∏≤‡∏á class ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏° RAG functionality\n",
        "   - ‡πÇ‡∏´‡∏•‡∏î Vector Store ‡πÅ‡∏•‡∏∞ LLM\n",
        "   - ‡∏™‡∏£‡πâ‡∏≤‡∏á QA ‡πÅ‡∏•‡∏∞ Conversation chains\n",
        "\n",
        "3. **Streamlit UI** üé®\n",
        "   - ‡∏™‡∏£‡πâ‡∏≤‡∏á Chat Interface\n",
        "   - Sidebar ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤\n",
        "   - Quick Questions ‡πÅ‡∏•‡∏∞ Source Citations\n",
        "\n",
        "4. **Deployment** üöÄ\n",
        "   - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå streamlit_app.py\n",
        "   - ‡∏ß‡∏¥‡∏ò‡∏µ‡∏£‡∏±‡∏ô Streamlit App\n",
        "   - ‡πÉ‡∏ä‡πâ uv ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ô (‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)\n",
        "\n",
        "### ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á:\n",
        "- `streamlit_app.py` - Streamlit application ‡∏´‡∏•‡∏±‡∏Å\n",
        "\n",
        "### ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà 5:\n",
        "‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ Advanced Topics ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏£‡∏∞‡∏ö‡∏ö\n",
        "\n",
        "---\n",
        "\n",
        "**‚è∞ ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: 60 ‡∏ô‡∏≤‡∏ó‡∏µ**\n",
        "**üìö ‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πà‡∏≠‡πÑ‡∏õ: `05_wrapup_advanced_topics.ipynb`**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
